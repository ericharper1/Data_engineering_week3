DataEng: Data Transport Activity
[this lab activity references tutorials at confluence.com]
Make a copy of this document and use it to record your results. Store a PDF copy of the document in your git repository along with your code before submitting for this week. For your code, you create several producer/consumer programs or you might make various features within one program. There is no one single correct way to do it. Regardless, store your code in your repository.

The goal for this week is to gain experience and knowledge of using a streaming data transport system (Kafka). Complete as many of the following exercises as you can. Proceed at a pace that allows you to learn and understand the use of Kafka with python. 
A. Initialization
Get your cloud.google.com account up and running
Redeem your GCP coupon
Login to your GCP console
Create a new, separate VM instance
Follow the Kafka tutorial from project assignment #1
Create a separate topic for this in-class activity
Make it “small” as you will not want to use many resources for this activity. By “small” I mean that you should choose medium or minimal options when asked for any configuration decisions about the topic, cluster, partitions, storage, anything. GCP/Confluent will ask you to choose the configs, and because you are using a free account you should opt for limited resources where possible.  
Get a basic producer and consumer working with a Kafka topic as described in the tutorials.
Create a sample breadcrumb data file (named bcsample.json) consisting of a sample of 1000 breadcrumb records. These can be any records because we will not be concerned with the actual contents of the breadcrumb records during this assignment.
Update your producer to parse your sample.json file and send its contents, one record at a time, to the kafka topic.
Use your consumer.py program (from the tutorial) to consume your records.
Got to this step. Having some syntax issues in consumer.py...

B. Kafka Monitoring
Find the Kafka monitoring console for your topic. Briefly describe its contents. Do the measured values seem reasonable to you?
Use this monitoring feature as you do each of the following exercises.

C. Kafka Storage
Run the linux command “wc bcsample.json”.  Record the output here so that we can verify that your sample data file is of reasonable size.

What happens if you run your consumer multiple times while only running the producer once?
Before the consumer runs, where might the data go, where might it be stored?
Is there a way to determine how much data Kafka/Confluent is storing for your topic? Do the Confluent monitoring tools help with this?
Create a “topic_clean.py” consumer that reads and discards all records for a given topic. This type of program can be very useful during debugging.
D. Multiple Producers
Clear all data from the topic
Run two versions of your producer concurrently, have each of them send all 1000 of your sample records. When finished, run your consumer once. Describe the results.
E. Multiple Concurrent Producers and Consumers
Clear all data from the topic
Update your Producer code to include a 250 msec sleep after each send of a message to the topic.
Run two or three concurrent producers and two concurrent consumers all at the same time.
Describe the results.
F. Varying Keys
Clear all data from the topic

So far you have kept the “key” value constant for each record sent on a topic. But keys can be very useful to choose specific records from a stream.
 
Update your producer code to choose a random number between 1 and 5 for each record’s key.
Modify your consumer to consume only records with a specific key (or subset of keys).
Attempt to consume records with a key that does not exist. E.g., consume records with key value of “100”. Describe the results
Can you create a consumer that only consumes specific keys? If you run this consumer multiple times with varying keys then does it allow you to consume messages out of order while maintaining order within each key?
G. Producer Flush
The provided tutorial producer program calls “producer.flush()” at the very end, and presumably your new producer also calls producer.flush(). 
What does Producer.flush() do? 
What happens if you do not call producer.flush()?  
What happens if you call producer.flush() after sending each record?
What happens if you wait for 2 seconds after every 5th record send, and you call flush only after every 15 record sends, and you have a consumer running concurrently?  Specifically, does the consumer receive each message immediately? only after a flush? Something else?

H. Consumer Groups
Create two consumer groups with one consumer program instance in each group.
Run the producer and have it produce all 1000 messages from your sample file.
Run each of the consumers and verify that each consumer consumes all of the 50 messages.
Create a second consumer within one of the groups so that you now have three consumers total.
Rerun the producer and consumers. Verify that each consumer group consumes the full set of messages but that each consumer within a consumer group only consumes a portion of the messages sent to the topic.

I. Kafka Transactions
Create a new producer, similar to the previous producer, that uses transactions.
The producer should begin a transaction, send 4 records in the transactions, then wait for 2 seconds, then choose True/False randomly with equal probability. If True then finish the transaction successfully with a commit.  If False is picked then cancel the transaction. 
Create a new transaction-aware consumer. The consumer should consume the data. It should also use the Confluent/Kaka transaction API with a “read_committed” isolation level. (I can’t find evidence of other isolation levels). 
Transaction across multiple topics. Create a second topic and modify your producer to send two records to the first topic and two records to the second topic before randomly committing or canceling the transaction. Modify the consumer to consume from the two queues. Verify that it only consumes committed data and not uncommitted or canceled data.

